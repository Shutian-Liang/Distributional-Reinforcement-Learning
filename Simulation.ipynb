{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional reinforment learning simulation for CDLab\n",
    "by *Shutian @ CDLab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from ipywidgets import interactive, fixed\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main content includes four parts:\n",
    "- The convergence of Temporal difference(TD0)\n",
    "- Why mean representation may not good\n",
    "- TD Learning modification\n",
    "- Dopamine Neuron firing traits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning\n",
    "\n",
    "Temporal difference is the based on model-free framework to learn the state or action value. Different from Monte-carlo learning, temporal difference learning does backup immediately after arriving the next state and receiving the reward. The most famaliar formula is the Rescorla-Wagner rule which can be seen as the $TD_0$ with discount parameter set 0. For similarity, we write this formula as follows:$$ V_{t+1} = V_t + \\alpha\\delta_t,\\  \\ where\\ \\delta_t = r - V_t\\ as\\ the\\ prediction\\ error$$\n",
    "\n",
    "When sample suffcient episode and set learning rate close to 0, the V will converg to the expectation of the reward distribution. Examples are as follows:\n",
    "\n",
    "Here we use bandit task and set the reward distribution as Gaussian Distribution, we sample for 1e5 times and see the final value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ec728cf5f648cb8073f84e5b38a892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='normal', description='distribution'), IntSlider(value=6, description='arg1',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class bandits:\n",
    "    def __init__(self, distribution, arg1,arg2):\n",
    "        self.distribution = getattr(np.random, distribution)\n",
    "        self.loc = arg1\n",
    "        self.scale = arg2\n",
    "        \n",
    "    def sample(self):\n",
    "        # sample one reward from the distribution\n",
    "        reward = self.distribution(self.loc,self.scale)\n",
    "        return reward\n",
    "    \n",
    "def TD_Update(V,reward,lrp,lrn):\n",
    "    \"\"\" Traditional Temporal Difference Learning\n",
    "    \"\"\"\n",
    "    V += lrp*(reward-V)\n",
    "    return V \n",
    " \n",
    "globals_dict = globals()  \n",
    "def PlotStateValue(distribution = 'normal',arg1=6,arg2=2,lrp = 0.01,lrn=  0.01,UpdateRule='TD_Update'):\n",
    "    \"\"\"Plot the estimated state value \n",
    "\n",
    "    Args:\n",
    "        distribution (str, optional): _description_. Defaults to 'normal'.\n",
    "        arg1 (int, optional): _description_. Defaults to 6.\n",
    "        arg2 (int, optional): _description_. Defaults to 2.\n",
    "    \"\"\"\n",
    "    bandit = bandits(distribution,arg1,arg2)\n",
    "    trials,V = 5000, 0\n",
    "    V_trajectory = [0]\n",
    "    x = np.arange(0,trials+1)\n",
    "    UpdateRule = globals_dict[UpdateRule]\n",
    "    for _ in range(trials):\n",
    "        reward = bandit.sample()\n",
    "        V = UpdateRule(V,reward,lrp,lrn)\n",
    "        V_trajectory.append(V)\n",
    "    plt.plot(x,np.array(V_trajectory))\n",
    "    plt.ylabel('Estimated Reward')\n",
    "    plt.xlabel('Trials')\n",
    "    \n",
    "    # add horizitontal line of different distribution\n",
    "    # For Gaussian, we draw the mu \n",
    "    # For Gamma, we draw mean, median and mode \n",
    "    x1 = [0,trials]\n",
    "    if distribution == 'normal':\n",
    "        plt.plot(x1,[arg1,arg1],color='orange',ls='--',label='Mean')\n",
    "    elif distribution == 'gamma':\n",
    "        # this part for TD_binary\n",
    "        mean = arg1*arg2\n",
    "        mode = (arg1-1)*arg2\n",
    "        dis = stats.gamma(arg1,scale = arg2)\n",
    "        median = dis.ppf(0.5)\n",
    "        plt.plot(x1,[mean,mean],color='orange',ls='--',label='Mean')\n",
    "        plt.plot(x1,[mode,mode],color='green',ls='--',label='Mode')\n",
    "        plt.plot(x1,[median,median],color='red',ls='--',label='Median')\n",
    "        \n",
    "        # thie for quantile TD\n",
    "        if lrp != lrn:\n",
    "            tau = lrp/(lrp+lrn)\n",
    "            quantile = dis.ppf(tau)\n",
    "            plt.plot(x1,[quantile,quantile],color='purple',ls='--',label=f'{round(tau,2)} quantile')\n",
    "            \n",
    "    ax=plt.gca()  \n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    plt.legend()\n",
    "    plt.title('Estimated changes with trials')\n",
    "    plt.show()\n",
    "\n",
    "widget1=interactive(PlotStateValue, arg1=(4, 10, 1), arg2=(1, 5, 0.5),UpdateRule = fixed('TD_Update'),lrp=fixed(0.01),lrn=fixed(0.01))\n",
    "widget1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does expectation represent everything ?\n",
    "\n",
    "Though using traditional temporal difference learning meathod we can approximate the expectation of the reward distribution, does the expectation of the distribution mean everything? In other words, does knowing the expectation really help us make the optimal choice ?\n",
    "\n",
    "The answer is not, please see the next example below.In this case, I will construct two distribution with the same expectation and variance. Using traditional reinforcement learning, we can converge to the same expectation. But if you think they're just as good, then you are completely wrong !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of calculation, I choose distributions that have analytical solutions of mean and standard variance. For symmetrical distribution, I choose the Gaussian Distribution: $$p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp({-\\frac{(x-\\mu)^2}{2\\sigma^2})},where\\ \\mu\\ is\\ the\\ mean,\\ and\\ \\sigma^2\\ is\\ the\\ variance$$ \n",
    "\n",
    "For skewed distribution, I choose Gamma Distribution:$$p(x;\\beta,\\alpha) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}x^{\\alpha-1}\\exp(-\\frac{x}{\\beta}),x>0, where\\ \\Gamma(\\alpha) = \\int_{0}^{\\infty}t^{\\alpha-1}\\exp{-t}dt$$\n",
    "For Gamma Distribution, we can give the mean and variance directly: \n",
    "$$\\begin{align*}\n",
    "    \\mu &= \\alpha\\beta \\\\\n",
    "    \\sigma^2 &= \\alpha\\beta^2 \\\\\n",
    "    Mode &= (\\alpha-1)\\beta\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Next we show the probability density distribution of Gaussian Distribution($\\mu = 16,\\sigma = 4\\sqrt{2}$) and Gamma Distribution($\\alpha = 8 ,\\beta=2$), so the mean and variance of the two distribution are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace1a3a4b0da45e289c4382d5fc7cc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=8, description='alpha', max=10, min=4), FloatSlider(value=2.0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def PlotDistribution(alpha=8,beta=2):\n",
    "    # set the parameter\n",
    "    mu, sigma = alpha*beta,np.sqrt(alpha*beta**2)\n",
    "    mode = (alpha-1)*beta\n",
    "\n",
    "    # show the PDF\n",
    "    x = np.arange(mu-20,mu+20,0.01)\n",
    "    p_gaussian = stats.norm.pdf(x,mu,sigma)\n",
    "    p_gamma = stats.gamma.pdf(x,alpha,scale=beta)\n",
    "    PdfMode = stats.gamma.pdf(mode,alpha,scale=beta)\n",
    "    PdfMean = stats.norm.pdf(mu,mu,sigma)\n",
    "\n",
    "    # draw the PDF\n",
    "    plt.plot(x,p_gaussian,label='Gaussian Distribution')\n",
    "    plt.plot(x,p_gamma,label='Gamma Distribution')\n",
    "\n",
    "    # label the mean median and mode\n",
    "    plt.plot([mode,mode],[0,PdfMode],color='gray',ls='--',label='Mode')\n",
    "    plt.plot([mu,mu],[0,PdfMean],color='gray',ls='-',label='Mean')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    ax=plt.gca()  \n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.title('PDF of Gaussian and Gamma Distribution')\n",
    "    plt.show()\n",
    "    \n",
    "widget2 = interactive(PlotDistribution, alpha=(4, 10, 1), beta=(0.2, 10, 0.5))\n",
    "widget2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c7b7a9fada4a458badb29271686a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=8, description='alpha', max=10, min=4), FloatSlider(value=2.0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample enough episode and try to see which one is better\n",
    "def Contrast(alpha=8,beta=2):\n",
    "    mu, sigma = alpha*beta,np.sqrt(alpha*beta**2)\n",
    "    iterations = 200000\n",
    "    num = 0\n",
    "    for i in range(int(iterations)):\n",
    "        value_gaussian = np.random.normal(mu,sigma)\n",
    "        value_gamma = np.random.gamma(alpha,beta)\n",
    "        num += (value_gamma>=value_gaussian)\n",
    "    print(num/iterations)\n",
    "widget3 = interactive(Contrast, alpha=(4, 10, 1), beta=(0.2, 10, 0.5))\n",
    "widget3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulations are as above and we can see that the probability of gamma reward better than gaussian reward is lower than 0.5, which means that given the same expectation and variance we can't make the best choice and we need to know more information about the distribution. And the full mathmetical proof of why gaussian is better than gamma is as follow and I try to give the closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-Learning modification\n",
    "\n",
    "In this part, I will give some formats of new TD learning and the modifications are mainly focused on learning rate and prediction error. I refer to the [Lowet et al., 2020] and give 3 types of updated rule. Details and the effect of updated rule are combined with example.\n",
    "\n",
    "(1) Binarize rule\n",
    "This update method will converge to the median of the distribution. The full mathematical proof is not given here but we can imagine when this equation gets to the equilibrium. If this equation gets the balance which means that the estimated V jitters around a certain value and the prediction error sometimes positive and sometimes negative with the equal probability. So the certain value must be the median of the distribution.\n",
    "\n",
    "$$V = \n",
    "V + \\alpha\n",
    "\\begin{cases}\n",
    "-1 \\ \\text{  if } \\delta \\leq 0 \\\\\n",
    "1 \\ \\text{  if } \\delta \\gt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Considering the distinction between the expectation and the median of one distribution，I still take the skewed distribution-gamma distribution for example. Though no clear closed form solution of median, python scipy moudle provides the `stats.gamma.ppf` function to do value approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aca14f91abe40fdb133737618f85138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='gamma', description='distribution'), IntSlider(value=6, description='arg1', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Binarize rule simulation\n",
    "\n",
    "# update rule\n",
    "def TD_UpdateBinary(value,reward,lrp,lrn):\n",
    "    RPE = reward - value\n",
    "    error = -1 if RPE <= 0 else 1\n",
    "    value += lrp*error\n",
    "    return value\n",
    "\n",
    "widget4 = interactive(PlotStateValue,distribution='gamma',arg1=(4, 10, 1), arg2=(1, 5, 0.5),UpdateRule = 'TD_UpdateBinary',lrp=fixed(0.01),lrn=fixed(0.01))\n",
    "widget4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Seperate learning rate but error binary(quantitle regression): In this update rule, we add variability in the learning rate and set two separate learning rate, in which $\\alpha^+$ for positive prediction error and $\\alpha^-$ for negative prediction error. Its formula is bolow:\n",
    "$$V = \n",
    "V + \n",
    "\\begin{cases}\n",
    "\\alpha^-*(-1) \\ \\text{  if } \\delta \\leq 0 \\\\\n",
    "\\alpha^+*(+1) \\ \\text{  if } \\delta \\gt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For any combination of $\\alpha^+$ and $\\alpha^-$, a value predictor that learns according to the above rule will converge to the $\\frac{\\alpha^+}{\\alpha^++\\alpha^-}=\\tau$=$\\tau$-th quantile. For example, when $\\alpha^+$ = $\\alpha^-$, it converges to the median of the distribution. If you want to see the mathematic details, please look forward to the [quantile regression](https://en.wikipedia.org/wiki/Quantile_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e18613359334efcb26e9b0faaee308b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='gamma', description='distribution'), IntSlider(value=6, description='arg1', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def TD_UpdateQuantile(value,reward,lrp,lrn):\n",
    "    RPE = reward - value\n",
    "    error = -1 if RPE <= 0 else 1\n",
    "    if error == 1:\n",
    "        value += lrp*error\n",
    "    else:\n",
    "        value += lrn*error\n",
    "    return value\n",
    "\n",
    "widget5 = interactive(PlotStateValue,distribution='gamma',arg1=(4, 10, 1), arg2=(1, 5, 0.5),lrp=(0.01,0.1,0.01),lrn=(0.01,0.1,0.01),UpdateRule = 'TD_UpdateQuantile')\n",
    "widget5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Seperate learning rate and prediction error(Expectile regression):One can also consider a family of value predictors that retains the original form of the update rule and with different learning rate. Actually there exists this kind of update rule as follows:\n",
    "$$V = \n",
    "V + \n",
    "\\begin{cases}\n",
    "\\alpha^-*\\delta \\ \\text{  if } \\delta \\leq 0 \\\\\n",
    "\\alpha^+*\\delta \\ \\text{  if } \\delta \\gt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This update rule gives rise to a range of value estimates called expectiles, which generalize the mean just as quantiles generalize the median. However, unlike quantiles, expectiles do not bear a straightforward relationship to the CDF. To understand them, it is necessary to adopt a more general perspective on learning. For math details, please refer to [expectile regression](https://epub.ub.uni-muenchen.de/31542/1/1471082x14561155.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f02cede335f48b09a8f03d27ffda6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='gamma', description='distribution'), IntSlider(value=6, description='arg1', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def TD_UpdateExpectile(value,reward,lrp,lrn):\n",
    "    RPE = reward - value\n",
    "    if RPE >0:\n",
    "        value += lrp*RPE\n",
    "    else:\n",
    "        value += lrn*RPE\n",
    "    return value\n",
    "\n",
    "widget5 = interactive(PlotStateValue,distribution='gamma',arg1=(4, 10, 1), arg2=(1, 5, 0.5),lrp=(0.01,0.1,0.01),lrn=(0.01,0.1,0.01),UpdateRule = 'TD_UpdateExpectile')\n",
    "widget5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dopamine Neuron Simulation\n",
    "\n",
    "In this part I will simulate the dopamine neuron spking neural dynamics when encourting the reward for the reason to illustrate why they will response to the cue aftering learning the correlation between cue and reward. Most of us know that the dopamine may reflect the reward prediction error just like the schulz et al., 1997 shows. However we may ignore the intense spiking after the cue which predicting the reward in the near future.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"pictures/dopamine1997.png\" alt=\"dopamine1997\">\n",
    "</p>\n",
    "\n",
    "Most of us just focus on the Temporal Difference rule but we miss the math detail of the TD learning in dopamine coding. Next I will reveal the math details and show how the TD error does backpropagation.\n",
    "\n",
    "$$\\delta(t) = r(t) + \\gamma\\hat{V}(t+1)-\\hat{V}(t)$$\n",
    "$$\\hat{V}(t) = \\sum_iw_ix_i(t)$$\n",
    "$$\\Delta w_i = \\alpha \\sum_t x_i(t)\\delta(t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21521694c1104fdfbd513cdb0aeeed5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='trials', max=800, min=1, step=10), FloatSlider(value=0.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def PlotTDError(trials=1,lrp=0.01,lrn=0.01,gamma=1,magnitudes=[10],dis = False):\n",
    "    \"\"\"Plot the TD error backpropagation\n",
    "\n",
    "    Args:\n",
    "        trials (int, optional): _how many trials the neuron does_. Defaults to 1000.\n",
    "        lrp (float, optional): _positive learning rate_. Defaults to 0.01.\n",
    "        lrn (float, optional): _negative learning rate_. Defaults to 0.01.\n",
    "        gamma (int, optional): _discount parameter_. Defaults to 0.\n",
    "        magnitude (int, optional): _the reward magnitude_. Defaults to 2.\n",
    "    \"\"\"\n",
    "    # Here we set time length for 30, what happens in the 30 seconds are as follows\n",
    "    # 0-4 nothing; 5:14 cue; 15-24 delay; 25 reward; 26-34 nothing\n",
    "    stimulus,R = np.zeros(35),np.zeros(35)\n",
    "    is_delay = np.zeros(35)\n",
    "    is_delay[15:26] = 1 \n",
    "    stimulus[5:14] = 1\n",
    "\n",
    "    # initialize the state value and weight\n",
    "    for magnitude in magnitudes:\n",
    "        V = np.zeros(36)\n",
    "        E = np.zeros(35)\n",
    "        for i in range(trials):\n",
    "            if dis == False:\n",
    "                R[25] = magnitude\n",
    "            else:\n",
    "                R[25] = np.random.normal(magnitude,2)\n",
    "            if lrp == lrn:\n",
    "                for t in range(35):\n",
    "                    if is_delay[t] == 1:\n",
    "                        E[t] = R[t]+gamma*V[t+1]-V[t]\n",
    "                        V[t] += lrp*E[t] \n",
    "            else:\n",
    "                for t in range(35):\n",
    "                    if is_delay[t] == 1:\n",
    "                        E[t] = R[t]+gamma*V[t+1]-V[t]\n",
    "                        if E[t] <= 0:\n",
    "                            V[t] += lrn*E[t]\n",
    "                        else:\n",
    "                            V[t] += lrp*E[t]     \n",
    "        x = np.arange(35)\n",
    "        plt.plot(x,E,label=f'magnitude = {magnitude}')\n",
    "    \n",
    "    #draw pictures\n",
    "    ax=plt.gca()  \n",
    "    plt.title('TD error changes with time')\n",
    "    plt.xlabel('Time')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "widget7 = interactive(PlotTDError,lrp=(0.01,0.1,0.01),lrn=(0.01,0.1,0.01),trials=(1,800,10),gamma=fixed(0.99),magnitudes=fixed([10,20]))\n",
    "widget7   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How to combine dopamine neuron firing traits with the distributional reinforment learning algorithm? In other words, how can we judge that the dopamine neuron do the distributional coding ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
